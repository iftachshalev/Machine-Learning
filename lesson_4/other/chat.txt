16:04:56	 From  מוטי מצה : תשלחו לכאן נוכחות
16:05:00	 From  Ofer Dayan : https://docs.google.com/forms/d/e/1FAIpQLSdmAl26mYW_gxAMulxhiLkYXOnCIJbFsLU5TYwwGuTWu8rIGA/viewform
16:05:01	 From  irene kipnis : https://forms.gle/QUw4oVumnqMmgMjh7
16:15:47	 From  נאדרה בשארה סאלח נאסר : What Is Bias?Bias is simply a constant value (or a constant vector) that is added to the product of inputs and weights. Bias is utilised to offset the result.The bias is used to shift the result of activation function towards the positive or negative side.Imagine this scenario:Let’s assume you want your neural network to return 2 when the input is 0. As the sum of product of weight and input is going to be 0, how will you ensure the neuron of the network returns 2?You can add a bias of 2.If we do not include the bias then the neural network is simply performing a matrix multiplication on the inputs and weights. This can easily end up over-fitting the data set.The addition of bias reduces the variance and hence introduces flexibility and better generlisation to the neural network.Bias is essentially the negative of the threshold, therefore the value of bias controls when to activate the activation function.If you want to understand what activation functions are then please read:
16:15:50	 From  נאדרה בשארה סאלח נאסר : https://medium.com/fintechexplained/neural-networks-bias-and-weights-10b53e6285da
16:33:28	 From  יקיר בשארי : scikit-learn
16:33:55	 From  Zvika Raviv : pip3 install -U scikit-learn
16:34:08	 From  עבד אל לטיף וותד : Scikit-learn
16:46:24	 From  סמי אבושח : https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html
16:54:10	 From  סמי אבושח : https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e
17:10:14	 From  דורית בן-דוד : המצגות והתרגול נמצאות פה: https://drive.google.com/drive/u/1/folders/1BDLpu5tvs2q3-0I4jqyWgNxom27nBvX0
17:39:15	 From  נאדרה בשארה סאלח נאסר : import numpy as npimport sklearn.neural_network inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])outputs = np.array([0, 1, 1, 0])    #XORmodel = sklearn.neural_network.MLPClassifier(                                             activation='logistic',                                            max_iter=100,                                             hidden_layer_sizes=(4,2),                                             solver='lbfgs')count =0iteration=10for i in range(iteration): model.fit(inputs,outputs) if np.array_equal(model.predict(inputs),outputs):    count +=1print("count of correct {0} out of {1}   ".format(count,iteration))
